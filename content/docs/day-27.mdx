---
title: "Day 27: Advanced Database Management"
description: "Master advanced database architecture, performance optimization, high availability, security, and automation best practices"
---

# Day 27: Advanced Database Management

## Learning Objectives

- Understand advanced database architecture and design patterns
- Master database performance optimization and tuning
- Learn high availability and disaster recovery strategies
- Explore database security and compliance
- Implement database automation and orchestration
- Understand database monitoring and observability

## Database Architecture Fundamentals

### Database Design Patterns

```yaml
# Database Architecture Patterns
apiVersion: v1
kind: ConfigMap
metadata:
  name: database-patterns
data:
  patterns: |
    # Master-Slave Replication
    master_slave:
      description: "Single master with multiple read replicas"
      components:
        - master: "Primary write node"
        - slaves: "Read-only replicas"
        - replication: "Asynchronous replication"
      use_cases:
        - "Read-heavy workloads"
        - "Geographic distribution"
        - "Backup and disaster recovery"

    # Multi-Master Replication
    multi_master:
      description: "Multiple write nodes with conflict resolution"
      components:
        - masters: "Multiple write nodes"
        - conflict_resolution: "Conflict detection and resolution"
        - consistency: "Eventual consistency"
      use_cases:
        - "High availability"
        - "Geographic distribution"
        - "Write scaling"

    # Sharding
    sharding:
      description: "Horizontal partitioning across multiple nodes"
      components:
        - shards: "Data partitions"
        - shard_key: "Partitioning strategy"
        - router: "Request routing"
      use_cases:
        - "Large datasets"
        - "Horizontal scaling"
        - "Performance optimization"

    # Microservices Database
    microservices_db:
      description: "Database per service pattern"
      components:
        - service_databases: "Independent databases per service"
        - data_ownership: "Service owns its data"
        - api_gateway: "Service communication"
      use_cases:
        - "Microservices architecture"
        - "Service independence"
        - "Technology diversity"
```

### Database Schema Design

```sql
-- Advanced Database Schema Design
-- Users and Authentication
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255) NOT NULL,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- User Profiles
CREATE TABLE user_profiles (
    user_id UUID PRIMARY KEY REFERENCES users(id) ON DELETE CASCADE,
    first_name VARCHAR(100),
    last_name VARCHAR(100),
    date_of_birth DATE,
    phone VARCHAR(20),
    address TEXT,
    preferences JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Products with Categories
CREATE TABLE categories (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    parent_id INTEGER REFERENCES categories(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE products (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    description TEXT,
    price DECIMAL(10,2) NOT NULL,
    category_id INTEGER REFERENCES categories(id),
    sku VARCHAR(50) UNIQUE,
    stock_quantity INTEGER DEFAULT 0,
    is_active BOOLEAN DEFAULT true,
    metadata JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Orders and Order Items
CREATE TABLE orders (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    status VARCHAR(50) DEFAULT 'pending',
    total_amount DECIMAL(10,2) NOT NULL,
    shipping_address JSONB,
    billing_address JSONB,
    payment_method VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE order_items (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    order_id UUID REFERENCES orders(id) ON DELETE CASCADE,
    product_id UUID REFERENCES products(id),
    quantity INTEGER NOT NULL,
    unit_price DECIMAL(10,2) NOT NULL,
    total_price DECIMAL(10,2) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Audit Trail
CREATE TABLE audit_logs (
    id SERIAL PRIMARY KEY,
    table_name VARCHAR(100) NOT NULL,
    record_id UUID,
    action VARCHAR(20) NOT NULL,
    old_values JSONB,
    new_values JSONB,
    user_id UUID REFERENCES users(id),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Indexes for Performance
CREATE INDEX idx_users_email ON users(email);
CREATE INDEX idx_users_username ON users(username);
CREATE INDEX idx_products_category ON products(category_id);
CREATE INDEX idx_products_sku ON products(sku);
CREATE INDEX idx_orders_user ON orders(user_id);
CREATE INDEX idx_orders_status ON orders(status);
CREATE INDEX idx_orders_created_at ON orders(created_at);
CREATE INDEX idx_order_items_order ON order_items(order_id);
CREATE INDEX idx_audit_logs_table_record ON audit_logs(table_name, record_id);
CREATE INDEX idx_audit_logs_created_at ON audit_logs(created_at);

-- Full-text search indexes
CREATE INDEX idx_products_search ON products USING gin(to_tsvector('english', name || ' ' || description));
```

## Database Performance Optimization

### Query Optimization

```sql
-- Query Optimization Examples

-- 1. Optimized JOIN with proper indexing
EXPLAIN ANALYZE
SELECT
    o.id as order_id,
    u.username,
    p.name as product_name,
    oi.quantity,
    oi.total_price
FROM orders o
JOIN users u ON o.user_id = u.id
JOIN order_items oi ON o.id = oi.order_id
JOIN products p ON oi.product_id = p.id
WHERE o.status = 'completed'
  AND o.created_at >= '2024-01-01'
ORDER BY o.created_at DESC
LIMIT 100;

-- 2. Window Functions for Analytics
SELECT
    category_id,
    product_name,
    price,
    ROW_NUMBER() OVER (PARTITION BY category_id ORDER BY price DESC) as price_rank,
    AVG(price) OVER (PARTITION BY category_id) as avg_category_price,
    price - AVG(price) OVER (PARTITION BY category_id) as price_diff_from_avg
FROM products
WHERE is_active = true;

-- 3. Materialized Views for Complex Queries
CREATE MATERIALIZED VIEW product_sales_summary AS
SELECT
    p.id as product_id,
    p.name as product_name,
    c.name as category_name,
    COUNT(oi.id) as total_orders,
    SUM(oi.quantity) as total_quantity,
    SUM(oi.total_price) as total_revenue,
    AVG(oi.unit_price) as avg_unit_price
FROM products p
JOIN categories c ON p.category_id = c.id
LEFT JOIN order_items oi ON p.id = oi.product_id
LEFT JOIN orders o ON oi.order_id = o.id
WHERE o.status = 'completed'
GROUP BY p.id, p.name, c.name;

-- Refresh materialized view
REFRESH MATERIALIZED VIEW product_sales_summary;

-- 4. Partitioned Tables for Large Datasets
CREATE TABLE orders_partitioned (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id UUID REFERENCES users(id),
    status VARCHAR(50) DEFAULT 'pending',
    total_amount DECIMAL(10,2) NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) PARTITION BY RANGE (created_at);

-- Create partitions by month
CREATE TABLE orders_2024_01 PARTITION OF orders_partitioned
FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');

CREATE TABLE orders_2024_02 PARTITION OF orders_partitioned
FOR VALUES FROM ('2024-02-01') TO ('2024-03-01');

-- 5. Optimized Aggregation Queries
SELECT
    DATE_TRUNC('month', created_at) as month,
    COUNT(*) as total_orders,
    SUM(total_amount) as total_revenue,
    AVG(total_amount) as avg_order_value,
    COUNT(DISTINCT user_id) as unique_customers
FROM orders
WHERE status = 'completed'
  AND created_at >= '2024-01-01'
GROUP BY DATE_TRUNC('month', created_at)
ORDER BY month;
```

### Database Tuning

```yaml
# PostgreSQL Configuration Optimization
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-config
data:
  postgresql.conf: |
    # Memory Configuration
    shared_buffers = 256MB                    # 25% of RAM
    effective_cache_size = 1GB                # 75% of RAM
    work_mem = 4MB                           # Per connection
    maintenance_work_mem = 64MB              # Maintenance operations

    # WAL Configuration
    wal_buffers = 16MB                       # WAL buffer size
    checkpoint_completion_target = 0.9        # Checkpoint completion
    wal_writer_delay = 200ms                 # WAL writer delay

    # Connection Configuration
    max_connections = 100                     # Maximum connections
    max_worker_processes = 8                 # Worker processes
    max_parallel_workers = 8                 # Parallel workers
    max_parallel_workers_per_gather = 4      # Workers per gather

    # Query Planner
    random_page_cost = 1.1                   # SSD cost
    effective_io_concurrency = 200           # Concurrent I/O
    default_statistics_target = 100          # Statistics target

    # Logging
    log_statement = 'all'                    # Log all statements
    log_min_duration_statement = 1000        # Log slow queries (ms)
    log_checkpoints = on                     # Log checkpoints
    log_connections = on                     # Log connections
    log_disconnections = on                  # Log disconnections
```

```sql
-- Database Performance Monitoring Queries

-- 1. Slow Query Analysis
SELECT
    query,
    calls,
    total_time,
    mean_time,
    rows,
    100.0 * shared_blks_hit / nullif(shared_blks_hit + shared_blks_read, 0) AS hit_percent
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 10;

-- 2. Index Usage Statistics
SELECT
    schemaname,
    tablename,
    indexname,
    idx_scan,
    idx_tup_read,
    idx_tup_fetch
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC;

-- 3. Table Statistics
SELECT
    schemaname,
    tablename,
    n_tup_ins as inserts,
    n_tup_upd as updates,
    n_tup_del as deletes,
    n_live_tup as live_rows,
    n_dead_tup as dead_rows,
    last_vacuum,
    last_autovacuum
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC;

-- 4. Connection Statistics
SELECT
    datname,
    numbackends,
    xact_commit,
    xact_rollback,
    blks_read,
    blks_hit,
    tup_returned,
    tup_fetched,
    tup_inserted,
    tup_updated,
    tup_deleted
FROM pg_stat_database;

-- 5. Lock Analysis
SELECT
    l.pid,
    l.mode,
    l.granted,
    d.datname,
    t.relname,
    a.usename
FROM pg_locks l
JOIN pg_database d ON l.database = d.oid
JOIN pg_class t ON l.relation = t.oid
JOIN pg_user a ON l.pid = a.usesysid
WHERE NOT l.granted;
```

## High Availability and Disaster Recovery

### Database Replication Setup

```yaml
# PostgreSQL Replication Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: postgres-replication
data:
  master.conf: |
    # Master Configuration
    wal_level = replica
    max_wal_senders = 10
    max_replication_slots = 10
    hot_standby = on
    archive_mode = on
    archive_command = 'test ! -f /var/lib/postgresql/archive/%f && cp %p /var/lib/postgresql/archive/%f'
    restore_command = 'cp /var/lib/postgresql/archive/%f %p'

  replica.conf: |
    # Replica Configuration
    hot_standby = on
    max_standby_archive_delay = 30s
    max_standby_streaming_delay = 30s
    wal_receiver_status_interval = 10s
    hot_standby_feedback = on
```

```sql
-- Replication Setup Commands

-- 1. Create Replication User
CREATE USER replicator REPLICATION LOGIN PASSWORD 'replication_password';

-- 2. Create Replication Slot
SELECT pg_create_physical_replication_slot('replica_1');

-- 3. Backup for Replica Setup
pg_basebackup -h master_host -D /var/lib/postgresql/data -U replicator -v -P -W

-- 4. Configure Replica
-- Add to postgresql.conf:
primary_conninfo = 'host=master_host port=5432 user=replicator password=replication_password'
primary_slot_name = 'replica_1'
restore_command = 'cp /var/lib/postgresql/archive/%f %p'
recovery_target_timeline = 'latest'

-- 5. Start Replica
pg_ctl start -D /var/lib/postgresql/data

-- 6. Monitor Replication
SELECT
    pid,
    application_name,
    client_addr,
    state,
    sent_lsn,
    write_lsn,
    flush_lsn,
    replay_lsn
FROM pg_stat_replication;
```

### Backup and Recovery

```bash
#!/bin/bash
# Database Backup and Recovery Script

# Configuration
DB_NAME="myapp"
DB_USER="postgres"
BACKUP_DIR="/var/backups/postgresql"
RETENTION_DAYS=30

# Create backup directory
mkdir -p $BACKUP_DIR

# Full backup
echo "Starting full backup..."
pg_dump -h localhost -U $DB_USER -d $DB_NAME \
    --format=custom \
    --compress=9 \
    --verbose \
    --file="$BACKUP_DIR/full_backup_$(date +%Y%m%d_%H%M%S).sql"

# WAL backup
echo "Starting WAL backup..."
pg_basebackup -h localhost -U $DB_USER \
    --pgdata="$BACKUP_DIR/wal_backup_$(date +%Y%m%d_%H%M%S)" \
    --format=tar \
    --gzip \
    --verbose

# Cleanup old backups
echo "Cleaning up old backups..."
find $BACKUP_DIR -name "*.sql" -mtime +$RETENTION_DAYS -delete
find $BACKUP_DIR -name "wal_backup_*" -mtime +$RETENTION_DAYS -exec rm -rf {} \;

echo "Backup completed successfully"
```

```sql
-- Point-in-Time Recovery

-- 1. Create Recovery Configuration
-- recovery.conf (PostgreSQL 12+)
restore_command = 'cp /var/lib/postgresql/archive/%f %p'
recovery_target_time = '2024-01-15 14:30:00'
recovery_target_action = 'promote'

-- 2. Start Recovery
pg_ctl start -D /var/lib/postgresql/data

-- 3. Monitor Recovery
SELECT pg_is_in_recovery();

-- 4. Promote to Primary (if needed)
SELECT pg_promote();
```

## Database Security

### Database Security Configuration

```sql
-- Database Security Implementation

-- 1. Create Secure Users and Roles
CREATE ROLE app_readonly;
CREATE ROLE app_readwrite;
CREATE ROLE app_admin;

-- Grant appropriate permissions
GRANT CONNECT ON DATABASE myapp TO app_readonly, app_readwrite, app_admin;
GRANT USAGE ON SCHEMA public TO app_readonly, app_readwrite, app_admin;

-- Read-only permissions
GRANT SELECT ON ALL TABLES IN SCHEMA public TO app_readonly;
GRANT SELECT ON ALL SEQUENCES IN SCHEMA public TO app_readonly;

-- Read-write permissions
GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO app_readwrite;
GRANT USAGE, SELECT ON ALL SEQUENCES IN SCHEMA public TO app_readwrite;

-- Admin permissions
GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO app_admin;
GRANT ALL PRIVILEGES ON ALL SEQUENCES IN SCHEMA public TO app_admin;

-- 2. Row Level Security (RLS)
ALTER TABLE orders ENABLE ROW LEVEL SECURITY;

CREATE POLICY user_orders_policy ON orders
    FOR ALL
    TO app_readwrite
    USING (user_id = current_setting('app.user_id')::UUID);

-- 3. Data Encryption
-- Create encrypted columns
ALTER TABLE users ADD COLUMN encrypted_ssn BYTEA;
ALTER TABLE users ADD COLUMN ssn_iv BYTEA;

-- 4. Audit Logging
CREATE OR REPLACE FUNCTION audit_trigger_function()
RETURNS TRIGGER AS $$
BEGIN
    IF TG_OP = 'INSERT' THEN
        INSERT INTO audit_logs (table_name, record_id, action, new_values, user_id)
        VALUES (TG_TABLE_NAME, NEW.id, 'INSERT', to_jsonb(NEW), current_setting('app.user_id')::UUID);
        RETURN NEW;
    ELSIF TG_OP = 'UPDATE' THEN
        INSERT INTO audit_logs (table_name, record_id, action, old_values, new_values, user_id)
        VALUES (TG_TABLE_NAME, NEW.id, 'UPDATE', to_jsonb(OLD), to_jsonb(NEW), current_setting('app.user_id')::UUID);
        RETURN NEW;
    ELSIF TG_OP = 'DELETE' THEN
        INSERT INTO audit_logs (table_name, record_id, action, old_values, user_id)
        VALUES (TG_TABLE_NAME, OLD.id, 'DELETE', to_jsonb(OLD), current_setting('app.user_id')::UUID);
        RETURN OLD;
    END IF;
    RETURN NULL;
END;
$$ LANGUAGE plpgsql;

-- Create audit triggers
CREATE TRIGGER audit_users_trigger
    AFTER INSERT OR UPDATE OR DELETE ON users
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();

-- 5. Connection Security
-- pg_hba.conf configuration
# TYPE  DATABASE        USER            ADDRESS                 METHOD
local   all             postgres                                peer
host    myapp           app_readonly    192.168.1.0/24         md5
host    myapp           app_readwrite   192.168.1.0/24         md5
host    myapp           app_admin       192.168.1.100/32       md5
host    all             all             0.0.0.0/0               reject
```

### Database Encryption

```python
# Database Encryption Implementation
import psycopg2
import cryptography
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
import base64
import os

class DatabaseEncryption:
    def __init__(self, master_key: str):
        self.master_key = master_key.encode()
        self.fernet = self._create_fernet()

    def _create_fernet(self) -> Fernet:
        """Create Fernet cipher from master key"""
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=b'database_salt',  # In production, use random salt
            iterations=100000,
        )
        key = base64.urlsafe_b64encode(kdf.derive(self.master_key))
        return Fernet(key)

    def encrypt_value(self, value: str) -> bytes:
        """Encrypt a string value"""
        return self.fernet.encrypt(value.encode())

    def decrypt_value(self, encrypted_value: bytes) -> str:
        """Decrypt an encrypted value"""
        return self.fernet.decrypt(encrypted_value).decode()

    def encrypt_column(self, table: str, column: str, id_column: str = 'id'):
        """Encrypt all values in a column"""
        conn = psycopg2.connect("dbname=myapp user=postgres")
        cursor = conn.cursor()

        # Get all values
        cursor.execute(f"SELECT {id_column}, {column} FROM {table} WHERE {column} IS NOT NULL")
        rows = cursor.fetchall()

        for row_id, value in rows:
            if value and not isinstance(value, bytes):  # Not already encrypted
                encrypted_value = self.encrypt_value(str(value))
                cursor.execute(f"UPDATE {table} SET {column} = %s WHERE {id_column} = %s",
                            (encrypted_value, row_id))

        conn.commit()
        cursor.close()
        conn.close()

    def decrypt_column(self, table: str, column: str, id_column: str = 'id'):
        """Decrypt all values in a column (for migration)"""
        conn = psycopg2.connect("dbname=myapp user=postgres")
        cursor = conn.cursor()

        # Get all encrypted values
        cursor.execute(f"SELECT {id_column}, {column} FROM {table} WHERE {column} IS NOT NULL")
        rows = cursor.fetchall()

        for row_id, encrypted_value in rows:
            if isinstance(encrypted_value, bytes):  # Encrypted
                decrypted_value = self.decrypt_value(encrypted_value)
                cursor.execute(f"UPDATE {table} SET {column} = %s WHERE {id_column} = %s",
                            (decrypted_value, row_id))

        conn.commit()
        cursor.close()
        conn.close()

# Usage
encryption = DatabaseEncryption("your-master-key-here")

# Encrypt sensitive data
encryption.encrypt_column("users", "encrypted_ssn")

# Decrypt for migration
encryption.decrypt_column("users", "encrypted_ssn")
```

## Database Automation

### Database Migration System

```python
# Database Migration System
import psycopg2
import yaml
import os
from datetime import datetime
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class Migration:
    id: str
    name: str
    sql_up: str
    sql_down: str
    created_at: datetime

class DatabaseMigration:
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.migrations_table = "schema_migrations"
        self._create_migrations_table()

    def _create_migrations_table(self):
        """Create migrations tracking table"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        cursor.execute(f"""
            CREATE TABLE IF NOT EXISTS {self.migrations_table} (
                id VARCHAR(255) PRIMARY KEY,
                name VARCHAR(255) NOT NULL,
                executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        """)

        conn.commit()
        cursor.close()
        conn.close()

    def load_migrations(self, migrations_dir: str) -> List[Migration]:
        """Load migrations from directory"""
        migrations = []

        for filename in sorted(os.listdir(migrations_dir)):
            if filename.endswith('.yaml'):
                with open(os.path.join(migrations_dir, filename), 'r') as f:
                    data = yaml.safe_load(f)

                migration = Migration(
                    id=data['id'],
                    name=data['name'],
                    sql_up=data['sql_up'],
                    sql_down=data['sql_down'],
                    created_at=datetime.fromisoformat(data['created_at'])
                )
                migrations.append(migration)

        return migrations

    def get_executed_migrations(self) -> List[str]:
        """Get list of executed migration IDs"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        cursor.execute(f"SELECT id FROM {self.migrations_table} ORDER BY executed_at")
        executed = [row[0] for row in cursor.fetchall()]

        cursor.close()
        conn.close()

        return executed

    def execute_migration(self, migration: Migration, direction: str = 'up'):
        """Execute a single migration"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        try:
            if direction == 'up':
                cursor.execute(migration.sql_up)
                cursor.execute(f"INSERT INTO {self.migrations_table} (id, name) VALUES (%s, %s)",
                            (migration.id, migration.name))
            else:
                cursor.execute(migration.sql_down)
                cursor.execute(f"DELETE FROM {self.migrations_table} WHERE id = %s",
                            (migration.id,))

            conn.commit()
            print(f"Executed migration: {migration.name}")

        except Exception as e:
            conn.rollback()
            print(f"Error executing migration {migration.name}: {e}")
            raise
        finally:
            cursor.close()
            conn.close()

    def migrate(self, target_version: str = None):
        """Run migrations up to target version"""
        migrations = self.load_migrations('migrations')
        executed = self.get_executed_migrations()

        # Filter pending migrations
        pending = [m for m in migrations if m.id not in executed]

        if target_version:
            # Find target migration
            target_migration = next((m for m in migrations if m.id == target_version), None)
            if not target_migration:
                raise ValueError(f"Target migration {target_version} not found")

            # Execute up to target
            for migration in pending:
                if migration.id == target_version:
                    break
                self.execute_migration(migration, 'up')
        else:
            # Execute all pending migrations
            for migration in pending:
                self.execute_migration(migration, 'up')

    def rollback(self, steps: int = 1):
        """Rollback specified number of migrations"""
        executed = self.get_executed_migrations()
        migrations = self.load_migrations('migrations')

        # Get migrations to rollback
        to_rollback = executed[-steps:] if steps > 0 else executed

        for migration_id in reversed(to_rollback):
            migration = next(m for m in migrations if m.id == migration_id)
            self.execute_migration(migration, 'down')

# Usage
migration_system = DatabaseMigration("postgresql://user:pass@localhost/myapp")

# Run all pending migrations
migration_system.migrate()

# Rollback last migration
migration_system.rollback(1)
```

### Database Monitoring

```python
# Database Monitoring System
import psycopg2
import time
import json
from typing import Dict, List
from dataclasses import dataclass
from datetime import datetime

@dataclass
class DatabaseMetrics:
    timestamp: datetime
    active_connections: int
    idle_connections: int
    total_connections: int
    cache_hit_ratio: float
    slow_queries: int
    deadlocks: int
    disk_usage_mb: float
    index_usage: Dict

class DatabaseMonitor:
    def __init__(self, connection_string: str):
        self.connection_string = connection_string
        self.metrics_history: List[DatabaseMetrics] = []

    def get_connection_stats(self) -> Dict:
        """Get database connection statistics"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        # Active connections
        cursor.execute("""
            SELECT count(*) as total_connections,
                   count(*) FILTER (WHERE state = 'active') as active_connections,
                   count(*) FILTER (WHERE state = 'idle') as idle_connections
            FROM pg_stat_activity
            WHERE datname = current_database()
        """)

        connection_stats = cursor.fetchone()

        cursor.close()
        conn.close()

        return {
            'total_connections': connection_stats[0],
            'active_connections': connection_stats[1],
            'idle_connections': connection_stats[2]
        }

    def get_cache_hit_ratio(self) -> float:
        """Get database cache hit ratio"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT
                sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as hit_ratio
            FROM pg_statio_user_tables
        """)

        hit_ratio = cursor.fetchone()[0] or 0.0

        cursor.close()
        conn.close()

        return hit_ratio

    def get_slow_queries(self) -> int:
        """Get count of slow queries"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT count(*)
            FROM pg_stat_statements
            WHERE mean_time > 1000
        """)

        slow_queries = cursor.fetchone()[0]

        cursor.close()
        conn.close()

        return slow_queries

    def get_deadlocks(self) -> int:
        """Get deadlock count"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT deadlocks
            FROM pg_stat_database
            WHERE datname = current_database()
        """)

        deadlocks = cursor.fetchone()[0]

        cursor.close()
        conn.close()

        return deadlocks

    def get_disk_usage(self) -> float:
        """Get database disk usage in MB"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT pg_database_size(current_database()) / 1024 / 1024 as size_mb
        """)

        size_mb = cursor.fetchone()[0]

        cursor.close()
        conn.close()

        return size_mb

    def get_index_usage(self) -> Dict:
        """Get index usage statistics"""
        conn = psycopg2.connect(self.connection_string)
        cursor = conn.cursor()

        cursor.execute("""
            SELECT
                schemaname,
                tablename,
                indexname,
                idx_scan,
                idx_tup_read,
                idx_tup_fetch
            FROM pg_stat_user_indexes
            ORDER BY idx_scan DESC
            LIMIT 10
        """)

        indexes = []
        for row in cursor.fetchall():
            indexes.append({
                'schema': row[0],
                'table': row[1],
                'index': row[2],
                'scans': row[3],
                'tuples_read': row[4],
                'tuples_fetched': row[5]
            })

        cursor.close()
        conn.close()

        return {'top_indexes': indexes}

    def collect_metrics(self) -> DatabaseMetrics:
        """Collect all database metrics"""
        connection_stats = self.get_connection_stats()
        cache_hit_ratio = self.get_cache_hit_ratio()
        slow_queries = self.get_slow_queries()
        deadlocks = self.get_deadlocks()
        disk_usage = self.get_disk_usage()
        index_usage = self.get_index_usage()

        metrics = DatabaseMetrics(
            timestamp=datetime.now(),
            active_connections=connection_stats['active_connections'],
            idle_connections=connection_stats['idle_connections'],
            total_connections=connection_stats['total_connections'],
            cache_hit_ratio=cache_hit_ratio,
            slow_queries=slow_queries,
            deadlocks=deadlocks,
            disk_usage_mb=disk_usage,
            index_usage=index_usage
        )

        self.metrics_history.append(metrics)
        return metrics

    def generate_report(self) -> Dict:
        """Generate database performance report"""
        if not self.metrics_history:
            return {}

        recent_metrics = self.metrics_history[-10:]  # Last 10 measurements

        avg_cache_hit_ratio = sum(m.cache_hit_ratio for m in recent_metrics) / len(recent_metrics)
        avg_connections = sum(m.total_connections for m in recent_metrics) / len(recent_metrics)
        total_slow_queries = sum(m.slow_queries for m in recent_metrics)
        total_deadlocks = sum(m.deadlocks for m in recent_metrics)

        return {
            "average_cache_hit_ratio": avg_cache_hit_ratio,
            "average_connections": avg_connections,
            "total_slow_queries": total_slow_queries,
            "total_deadlocks": total_deadlocks,
            "current_disk_usage_mb": recent_metrics[-1].disk_usage_mb,
            "measurements_count": len(self.metrics_history),
            "last_measurement": recent_metrics[-1].timestamp.isoformat()
        }

# Usage
monitor = DatabaseMonitor("postgresql://user:pass@localhost/myapp")

# Collect metrics for 1 minute
for i in range(60):
    metrics = monitor.collect_metrics()
    print(f"Cache Hit Ratio: {metrics.cache_hit_ratio:.2%}")
    print(f"Active Connections: {metrics.active_connections}")
    print(f"Slow Queries: {metrics.slow_queries}")
    time.sleep(1)

# Generate report
report = monitor.generate_report()
print(json.dumps(report, indent=2))
```

## Hands-on Exercises

### Exercise 1: Database Performance Tuning

```bash
# Create database performance tuning script
cat > db_tuning.sh << 'EOF'
#!/bin/bash

echo "=== Database Performance Tuning ==="

# Check current PostgreSQL configuration
echo "Current PostgreSQL Configuration:"
psql -c "SHOW shared_buffers;"
psql -c "SHOW effective_cache_size;"
psql -c "SHOW work_mem;"
psql -c "SHOW max_connections;"

# Analyze table statistics
echo "Analyzing table statistics..."
psql -c "ANALYZE;"

# Check for slow queries
echo "Slow queries (last 24 hours):"
psql -c "
SELECT query, calls, total_time, mean_time, rows
FROM pg_stat_statements
WHERE mean_time > 1000
ORDER BY mean_time DESC
LIMIT 10;
"

# Check index usage
echo "Index usage statistics:"
psql -c "
SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read
FROM pg_stat_user_indexes
ORDER BY idx_scan DESC
LIMIT 10;
"

# Check table statistics
echo "Table statistics:"
psql -c "
SELECT schemaname, tablename, n_live_tup, n_dead_tup, last_vacuum, last_autovacuum
FROM pg_stat_user_tables
ORDER BY n_live_tup DESC
LIMIT 10;
"
EOF

chmod +x db_tuning.sh
./db_tuning.sh
```

### Exercise 2: Database Backup and Recovery

```bash
# Create backup and recovery script
cat > db_backup.sh << 'EOF'
#!/bin/bash

# Database backup script
DB_NAME="myapp"
DB_USER="postgres"
BACKUP_DIR="/var/backups/postgresql"
DATE=$(date +%Y%m%d_%H%M%S)

echo "Starting database backup..."

# Create backup directory
mkdir -p $BACKUP_DIR

# Full backup
echo "Creating full backup..."
pg_dump -h localhost -U $DB_USER -d $DB_NAME \
    --format=custom \
    --compress=9 \
    --verbose \
    --file="$BACKUP_DIR/full_backup_$DATE.sql"

# WAL backup
echo "Creating WAL backup..."
pg_basebackup -h localhost -U $DB_USER \
    --pgdata="$BACKUP_DIR/wal_backup_$DATE" \
    --format=tar \
    --gzip \
    --verbose

# Verify backup
echo "Verifying backup..."
pg_restore -l "$BACKUP_DIR/full_backup_$DATE.sql" > /dev/null
if [ $? -eq 0 ]; then
    echo "Backup verification successful"
else
    echo "Backup verification failed"
    exit 1
fi

echo "Backup completed: $BACKUP_DIR/full_backup_$DATE.sql"
EOF

chmod +x db_backup.sh
./db_backup.sh
```

### Exercise 3: Database Security Audit

```bash
# Create database security audit script
cat > db_security_audit.sh << 'EOF'
#!/bin/bash

echo "=== Database Security Audit ==="

# Check for weak passwords
echo "Checking for weak passwords..."
psql -c "
SELECT usename, passwd IS NULL as no_password
FROM pg_user
WHERE passwd IS NULL OR passwd = 'md5' || md5('password');
"

# Check for excessive privileges
echo "Checking for excessive privileges..."
psql -c "
SELECT grantee, table_name, privilege_type
FROM information_schema.role_table_grants
WHERE grantee != 'postgres'
  AND privilege_type IN ('ALL', 'DELETE', 'UPDATE', 'INSERT')
ORDER BY grantee, table_name;
"

# Check for public access
echo "Checking for public access..."
psql -c "
SELECT schemaname, tablename, grantee, privilege_type
FROM information_schema.role_table_grants
WHERE grantee = 'PUBLIC'
ORDER BY schemaname, tablename;
"

# Check for unencrypted connections
echo "Checking connection security..."
psql -c "
SELECT datname, usename, client_addr, ssl
FROM pg_stat_ssl
JOIN pg_stat_activity ON pg_stat_ssl.pid = pg_stat_activity.pid
WHERE ssl = false;
"

# Check for audit logging
echo "Checking audit configuration..."
psql -c "SHOW log_statement;"
psql -c "SHOW log_min_duration_statement;"
psql -c "SHOW log_connections;"
psql -c "SHOW log_disconnections;"
EOF

chmod +x db_security_audit.sh
./db_security_audit.sh
```

## Assessment Questions

1. **What are the key database performance optimization techniques?**

   - Query optimization, indexing, connection pooling, caching, partitioning

2. **How do you implement database high availability?**

   - Replication, clustering, load balancing, automatic failover

3. **What are the best practices for database security?**

   - Access control, encryption, audit logging, regular updates, monitoring

4. **How do you handle database backups and recovery?**

   - Regular backups, point-in-time recovery, testing recovery procedures

5. **What are the key components of database automation?**
   - Migration systems, monitoring, alerting, automated scaling

## Additional Resources

- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [MySQL Documentation](https://dev.mysql.com/doc/)
- [MongoDB Documentation](https://docs.mongodb.com/)
- [Database Design Best Practices](https://www.cockroachlabs.com/docs/stable/)
- [Database Performance Tuning](https://use-the-index-luke.com/)

## Next Steps

Tomorrow we'll explore **Advanced Cloud Services**, covering:

- Cloud-native application development
- Serverless architecture and functions
- Cloud storage and data services
- Cloud networking and security
- Multi-cloud and hybrid cloud strategies
